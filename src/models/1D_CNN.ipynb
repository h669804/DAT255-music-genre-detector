{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamental imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-09T14:28:31.260490Z",
     "iopub.status.busy": "2025-04-09T14:28:31.260217Z",
     "iopub.status.idle": "2025-04-09T14:28:48.598168Z",
     "shell.execute_reply": "2025-04-09T14:28:48.597457Z",
     "shell.execute_reply.started": "2025-04-09T14:28:31.260468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.ndimage import uniform_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T14:28:48.599952Z",
     "iopub.status.busy": "2025-04-09T14:28:48.599384Z",
     "iopub.status.idle": "2025-04-09T14:28:48.603530Z",
     "shell.execute_reply": "2025-04-09T14:28:48.602666Z",
     "shell.execute_reply.started": "2025-04-09T14:28:48.599917Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "audiofile_path = '/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original/blues/blues.00000.wav' #change to your own path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an audiofile and plot time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T14:28:48.605248Z",
     "iopub.status.busy": "2025-04-09T14:28:48.604972Z",
     "iopub.status.idle": "2025-04-09T14:29:03.790437Z",
     "shell.execute_reply": "2025-04-09T14:29:03.789516Z",
     "shell.execute_reply.started": "2025-04-09T14:28:48.605220Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x, sr = librosa.load(audiofile_path, sr=22050)\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.ylabel('Amplitude')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.title('Raw time series')\n",
    "librosa.display.waveshow(x,sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now smooth the audio and plot again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T14:29:24.770095Z",
     "iopub.status.busy": "2025-04-09T14:29:24.769306Z",
     "iopub.status.idle": "2025-04-09T14:29:25.419745Z",
     "shell.execute_reply": "2025-04-09T14:29:25.418842Z",
     "shell.execute_reply.started": "2025-04-09T14:29:24.770062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "audio_data = uniform_filter1d(x, size=25, mode='nearest')\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.ylabel('Amplitude')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.title('Smoothed audio')\n",
    "librosa.display.waveshow(audio_data,sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we can see the smoothing made the temporal patterns more clear, hopefully removing noise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T14:29:28.514185Z",
     "iopub.status.busy": "2025-04-09T14:29:28.513892Z",
     "iopub.status.idle": "2025-04-09T14:29:28.532027Z",
     "shell.execute_reply": "2025-04-09T14:29:28.530871Z",
     "shell.execute_reply.started": "2025-04-09T14:29:28.514161Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y, sr = librosa.load(audiofile_path, sr=None)\n",
    "chunk_duration = 4\n",
    "overlap = 2\n",
    "\n",
    "chunk_samples = chunk_duration * sr\n",
    "overlap_samples = overlap * sr\n",
    "\n",
    "num_chunks = int(np.ceil((len(y) - chunk_samples) / (chunk_samples - overlap_samples))) + 1\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    start = i * (chunk_samples - overlap_samples)\n",
    "    end = start + chunk_samples\n",
    "\n",
    "    chunk = y[start:end]\n",
    "    smoothed_chunk = uniform_filter1d(chunk, size=101, mode='nearest')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T14:29:31.056548Z",
     "iopub.status.busy": "2025-04-09T14:29:31.056276Z",
     "iopub.status.idle": "2025-04-09T14:29:31.060316Z",
     "shell.execute_reply": "2025-04-09T14:29:31.059526Z",
     "shell.execute_reply.started": "2025-04-09T14:29:31.056526Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "main_dir = '/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original' # Change to appropriate path\n",
    "classes = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "CNN models trained on time series data can be victims of overfitting due to noisy data, we want to smooth this out using the technique showed earlier making the data less noisy.\n",
    "Preprocessing methods:\n",
    "Data augmentation with chunking\n",
    "Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T14:29:34.255323Z",
     "iopub.status.busy": "2025-04-09T14:29:34.255041Z",
     "iopub.status.idle": "2025-04-09T14:29:34.263639Z",
     "shell.execute_reply": "2025-04-09T14:29:34.262680Z",
     "shell.execute_reply.started": "2025-04-09T14:29:34.255300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage import uniform_filter1d  # For moving average smoothing\n",
    "\n",
    "def load_and_preprocess_data(main_dir, classes):\n",
    "    data = []\n",
    "    labels = []\n",
    "    sample_rate = 22050\n",
    "\n",
    "    for i_class, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(main_dir, class_name)\n",
    "        for filename in os.listdir(class_dir):\n",
    "            if filename.endswith('.wav'):\n",
    "                # Skip the corrupted file 'jazz.00054.wav'\n",
    "                if filename == 'jazz.00054.wav':\n",
    "                    continue\n",
    "                \n",
    "                file_path = os.path.join(class_dir, filename)\n",
    "                audio_data, _ = librosa.load(file_path, sr=sample_rate)  # Force 22,050 Hz\n",
    "                \n",
    "                # Step 1: Smoothing (before chunking)\n",
    "                window_size = 75  # Smoothing window, adjust as needed\n",
    "                audio_data = uniform_filter1d(audio_data, size=window_size, mode='nearest')\n",
    "\n",
    "                # Parameters\n",
    "                chunk_duration = 4\n",
    "                overlap = 2\n",
    "                chunk_samples = int(chunk_duration * sample_rate)\n",
    "                step = int(overlap * sample_rate)\n",
    "\n",
    "                # Trim or pad to exactly 30s\n",
    "                expected_samples = 30 * sample_rate\n",
    "                if len(audio_data) > expected_samples:\n",
    "                    audio_data = audio_data[:expected_samples]\n",
    "                elif len(audio_data) < expected_samples:\n",
    "                    audio_data = np.pad(audio_data, (0, expected_samples - len(audio_data)), 'constant')\n",
    "\n",
    "                # Exactly 14 chunks for 30s\n",
    "                num_chunks = (expected_samples - chunk_samples) // step + 1\n",
    "                for i in range(num_chunks):\n",
    "                    start = i * step\n",
    "                    end = start + chunk_samples\n",
    "                    chunk = audio_data[start:end]\n",
    "                    \n",
    "                    if len(chunk) == chunk_samples:  # Only append full chunks\n",
    "                        data.append(chunk)\n",
    "                        labels.append(i_class)\n",
    "\n",
    "    data = np.array(data)[..., np.newaxis]  # Shape: (num_chunks_total, 88200, 1)\n",
    "    labels = tf.keras.utils.to_categorical(labels, num_classes=len(classes))\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T14:29:37.190406Z",
     "iopub.status.busy": "2025-04-09T14:29:37.190128Z",
     "iopub.status.idle": "2025-04-09T14:30:13.326839Z",
     "shell.execute_reply": "2025-04-09T14:30:13.326107Z",
     "shell.execute_reply.started": "2025-04-09T14:29:37.190384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data, labels = load_and_preprocess_data(main_dir, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T14:30:13.328458Z",
     "iopub.status.busy": "2025-04-09T14:30:13.328126Z",
     "iopub.status.idle": "2025-04-09T14:30:14.704506Z",
     "shell.execute_reply": "2025-04-09T14:30:14.703848Z",
     "shell.execute_reply.started": "2025-04-09T14:30:13.328426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, Dense\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "\n",
    "initializer = VarianceScaling()\n",
    "\n",
    "optimizers = {\n",
    "    'Adam': Adam(learning_rate=4e-4),\n",
    "    'SGD': SGD(learning_rate=4e-2, momentum=0.9),\n",
    "    'RMSprop': RMSprop(learning_rate=4e-4)\n",
    "}\n",
    "\n",
    "models = {}\n",
    "\n",
    "def build_model(optimizer):\n",
    "    inputs = tf.keras.layers.Input((data.shape[1], data.shape[2]))  # 4s at 22,050 Hz\n",
    "\n",
    "    # First Conv1D block\n",
    "    x = tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu', kernel_initializer=initializer, kernel_regularizer=l2(0.0001))(inputs)\n",
    "    x = tf.keras.layers.MaxPooling1D(3)(x)\n",
    "    \n",
    "    # Second Conv1D block\n",
    "    x = tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu', kernel_initializer=initializer, kernel_regularizer=l2(0.0001))(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "    \n",
    "    # Third Conv1D block\n",
    "    x = tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu', kernel_initializer=initializer, kernel_regularizer=l2(0.0001))(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "    \n",
    "    #Fourth Block\n",
    "    x = tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu', kernel_initializer=initializer, kernel_regularizer=l2(0.0001))(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "    \n",
    "    # Fifth block\n",
    "    x = tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu', kernel_initializer=initializer, kernel_regularizer=l2(0.0001))(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "    \n",
    "    # Sixth block\n",
    "    x = tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu', kernel_initializer=initializer, kernel_regularizer=l2(0.0001))(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "    \n",
    "    # Seventh block\n",
    "    x = tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu', kernel_initializer=initializer, kernel_regularizer=l2(0.0001))(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "    \n",
    "    # Eighth block\n",
    "    x = tf.keras.layers.Conv1D(256, kernel_size=3, activation='relu', kernel_initializer=initializer, kernel_regularizer=l2(0.000125))(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "    \n",
    "    # Ninth block with GlobalAverageMaxpooling\n",
    "    x = tf.keras.layers.Conv1D(512, kernel_size=3, activation='relu', kernel_initializer=initializer, kernel_regularizer=l2(0.0001))(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    \n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "\n",
    "    # Build the model\n",
    "    model = tf.keras.models.Model(inputs, x)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T14:30:14.705982Z",
     "iopub.status.busy": "2025-04-09T14:30:14.705751Z",
     "iopub.status.idle": "2025-04-09T14:30:16.678421Z",
     "shell.execute_reply": "2025-04-09T14:30:16.677729Z",
     "shell.execute_reply.started": "2025-04-09T14:30:14.705963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for name, optimizer in optimizers.items():\n",
    "    models[name] = build_model(optimizer)\n",
    "\n",
    "print(models)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T14:30:16.679678Z",
     "iopub.status.busy": "2025-04-09T14:30:16.679367Z",
     "iopub.status.idle": "2025-04-09T14:30:18.331365Z",
     "shell.execute_reply": "2025-04-09T14:30:18.330302Z",
     "shell.execute_reply.started": "2025-04-09T14:30:16.679654Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T14:30:18.332814Z",
     "iopub.status.busy": "2025-04-09T14:30:18.332455Z",
     "iopub.status.idle": "2025-04-09T16:39:05.770137Z",
     "shell.execute_reply": "2025-04-09T16:39:05.769350Z",
     "shell.execute_reply.started": "2025-04-09T14:30:18.332754Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5, verbose=1)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    print('Training model:', name)\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        validation_data=(X_test, y_test), \n",
    "                        epochs=50, \n",
    "                        batch_size=64, \n",
    "                        callbacks=[early_stopping, reduce_lr])\n",
    "    results[name] = history\n",
    "    \n",
    "    # Save history\n",
    "    with open(f'/kaggle/working/{name}_history.pkl', 'wb') as file:\n",
    "        pickle.dump(history.history, file)\n",
    "    \n",
    "    # Save model\n",
    "    model.save(f'/kaggle/working/{name}_model.h5')\n",
    "    print(f\"Model and history for {name} saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T16:39:22.999126Z",
     "iopub.status.busy": "2025-04-09T16:39:22.998702Z",
     "iopub.status.idle": "2025-04-09T16:39:23.007647Z",
     "shell.execute_reply": "2025-04-09T16:39:23.006797Z",
     "shell.execute_reply.started": "2025-04-09T16:39:22.999087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_optimizer_comparison(histories_dict):\n",
    "    # Dictionary to store reformatted histories\n",
    "    reformatted_histories = {}\n",
    "    \n",
    "    # Convert the history objects to the format expected by the original function\n",
    "    for name, history_obj in histories_dict.items():\n",
    "        reformatted_histories[name] = {'history': history_obj.history}\n",
    "    \n",
    "    # Continue with your original function logic using the reformatted data\n",
    "    # Step 2: Plot training and validation metrics\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Validation Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for name, metrics in reformatted_histories.items():\n",
    "        plt.plot(metrics['history']['val_accuracy'], label=f\"{name} Val Acc\")\n",
    "    plt.title('Validation Accuracy per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 2: Validation Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for name, metrics in reformatted_histories.items():\n",
    "        plt.plot(metrics['history']['val_loss'], label=f\"{name} Val Loss\")\n",
    "    plt.title('Validation Loss per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T16:39:29.281256Z",
     "iopub.status.busy": "2025-04-09T16:39:29.280970Z",
     "iopub.status.idle": "2025-04-09T16:39:29.781840Z",
     "shell.execute_reply": "2025-04-09T16:39:29.780849Z",
     "shell.execute_reply.started": "2025-04-09T16:39:29.281234Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary with your model histories\n",
    "histories = {}\n",
    "\n",
    "# Load each history file\n",
    "for optimizer in [\"Adam\", \"SGD\", \"RMSprop\"]:\n",
    "    try:\n",
    "        # Open and load the pickle file\n",
    "        with open(f'/kaggle/working/{optimizer}_history.pkl', 'rb') as file:\n",
    "            # Create a simple object with a history attribute\n",
    "            history_obj = type('', (), {})()\n",
    "            history_obj.history = pickle.load(file)\n",
    "            \n",
    "            # Add to the dictionary\n",
    "            histories[optimizer] = history_obj\n",
    "            \n",
    "        print(f\"Successfully loaded history for {optimizer}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load history for {optimizer}: {e}\")\n",
    "\n",
    "plot_optimizer_comparison(histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T16:39:58.307505Z",
     "iopub.status.busy": "2025-04-09T16:39:58.307168Z",
     "iopub.status.idle": "2025-04-09T16:40:21.785688Z",
     "shell.execute_reply": "2025-04-09T16:40:21.784990Z",
     "shell.execute_reply.started": "2025-04-09T16:39:58.307475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Convert probabilities to class labels\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    predictions[name] = {\n",
    "        'predictons': y_pred,\n",
    "        'predicted_classes': y_pred_classes\n",
    "    }\n",
    "    \n",
    "# Convert one-hot true labels to class labels\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T16:43:59.284361Z",
     "iopub.status.busy": "2025-04-09T16:43:59.283922Z",
     "iopub.status.idle": "2025-04-09T16:44:01.349045Z",
     "shell.execute_reply": "2025-04-09T16:44:01.348156Z",
     "shell.execute_reply.started": "2025-04-09T16:43:59.284295Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "for name, y_pred in predictions.items():\n",
    "    print(classification_report(y_true_classes, y_pred['predicted_classes'], target_names=classes))\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true_classes, y_pred['predicted_classes'])\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(f\"Confusion Matrix  1D CNN with {name}\")\n",
    "    plt.savefig(f'1dcnn{name}_confusion_matrix.png')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 568973,
     "sourceId": 1032238,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
