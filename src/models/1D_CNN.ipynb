{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1032238,"sourceType":"datasetVersion","datasetId":568973}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fundamental imports","metadata":{}},{"cell_type":"code","source":"import os\nimport librosa\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom scipy.ndimage import uniform_filter1d","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:28:31.260217Z","iopub.execute_input":"2025-04-09T14:28:31.260490Z","iopub.status.idle":"2025-04-09T14:28:48.598168Z","shell.execute_reply.started":"2025-04-09T14:28:31.260468Z","shell.execute_reply":"2025-04-09T14:28:48.597457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"audiofile_path = '/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original/blues/blues.00000.wav'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:28:48.599384Z","iopub.execute_input":"2025-04-09T14:28:48.599952Z","iopub.status.idle":"2025-04-09T14:28:48.603530Z","shell.execute_reply.started":"2025-04-09T14:28:48.599917Z","shell.execute_reply":"2025-04-09T14:28:48.602666Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x, sr = librosa.load(audiofile_path, sr=22050)\nplt.figure(figsize=(14,5))\nplt.ylabel('Amplitude')\nplt.xlabel('Time (s)')\nplt.title('Raw time series')\nlibrosa.display.waveshow(x,sr=sr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:28:48.604972Z","iopub.execute_input":"2025-04-09T14:28:48.605248Z","iopub.status.idle":"2025-04-09T14:29:03.790437Z","shell.execute_reply.started":"2025-04-09T14:28:48.605220Z","shell.execute_reply":"2025-04-09T14:29:03.789516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"audio_data = uniform_filter1d(x, size=25, mode='nearest')\nplt.figure(figsize=(14,5))\nplt.ylabel('Amplitude')\nplt.xlabel('Time (s)')\nplt.title('Smoothed audio')\nlibrosa.display.waveshow(audio_data,sr=sr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:29:24.769306Z","iopub.execute_input":"2025-04-09T14:29:24.770095Z","iopub.status.idle":"2025-04-09T14:29:25.419745Z","shell.execute_reply.started":"2025-04-09T14:29:24.770062Z","shell.execute_reply":"2025-04-09T14:29:25.418842Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y, sr = librosa.load(audiofile_path, sr=None)\nchunk_duration = 4\noverlap = 2\n\nchunk_samples = chunk_duration * sr\noverlap_samples = overlap * sr\n\nnum_chunks = int(np.ceil((len(y) - chunk_samples) / (chunk_samples - overlap_samples))) + 1\n\nfor i in range(num_chunks):\n    start = i * (chunk_samples - overlap_samples)\n    end = start + chunk_samples\n\n    chunk = y[start:end]\n    smoothed_chunk = uniform_filter1d(chunk, size=101, mode='nearest')\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:29:28.513892Z","iopub.execute_input":"2025-04-09T14:29:28.514185Z","iopub.status.idle":"2025-04-09T14:29:28.532027Z","shell.execute_reply.started":"2025-04-09T14:29:28.514161Z","shell.execute_reply":"2025-04-09T14:29:28.530871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"main_dir = '/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original'\nclasses = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:29:31.056276Z","iopub.execute_input":"2025-04-09T14:29:31.056548Z","iopub.status.idle":"2025-04-09T14:29:31.060316Z","shell.execute_reply.started":"2025-04-09T14:29:31.056526Z","shell.execute_reply":"2025-04-09T14:29:31.059526Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing\nCNN models trained on time series data can be victims of overfitting due to noisy data, we want to smooth this out and make the data less noisy\nPreprocessing methods:\nData augmentation with chunking\nSmoothing","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport os\nimport librosa\nimport tensorflow as tf\nfrom scipy.ndimage import uniform_filter1d  # For moving average smoothing\n\ndef load_and_preprocess_data(main_dir, classes):\n    data = []\n    labels = []\n    sample_rate = 22050\n\n    for i_class, class_name in enumerate(classes):\n        class_dir = os.path.join(main_dir, class_name)\n        for filename in os.listdir(class_dir):\n            if filename.endswith('.wav'):\n                # Skip the corrupted file 'jazz.00054.wav'\n                if filename == 'jazz.00054.wav':\n                    continue\n                \n                file_path = os.path.join(class_dir, filename)\n                audio_data, _ = librosa.load(file_path, sr=sample_rate)  # Force 22,050 Hz\n                \n                # Step 1: Smoothing (before chunking)\n                window_size = 75  # Smoothing window, adjust as needed\n                audio_data = uniform_filter1d(audio_data, size=window_size, mode='nearest')\n\n                # Parameters\n                chunk_duration = 4\n                overlap = 2\n                chunk_samples = int(chunk_duration * sample_rate)\n                step = int(overlap * sample_rate)\n\n                # Trim or pad to exactly 30s\n                expected_samples = 30 * sample_rate\n                if len(audio_data) > expected_samples:\n                    audio_data = audio_data[:expected_samples]\n                elif len(audio_data) < expected_samples:\n                    audio_data = np.pad(audio_data, (0, expected_samples - len(audio_data)), 'constant')\n\n                # Exactly 14 chunks for 30s\n                num_chunks = (expected_samples - chunk_samples) // step + 1\n                for i in range(num_chunks):\n                    start = i * step\n                    end = start + chunk_samples\n                    chunk = audio_data[start:end]\n                    \n                    if len(chunk) == chunk_samples:  # Only append full chunks\n                        data.append(chunk)\n                        labels.append(i_class)\n\n    data = np.array(data)[..., np.newaxis]  # Shape: (num_chunks_total, 88200, 1)\n    labels = tf.keras.utils.to_categorical(labels, num_classes=len(classes))\n\n    return data, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:29:34.255041Z","iopub.execute_input":"2025-04-09T14:29:34.255323Z","iopub.status.idle":"2025-04-09T14:29:34.263639Z","shell.execute_reply.started":"2025-04-09T14:29:34.255300Z","shell.execute_reply":"2025-04-09T14:29:34.262680Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data, labels = load_and_preprocess_data(main_dir, classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:29:37.190128Z","iopub.execute_input":"2025-04-09T14:29:37.190406Z","iopub.status.idle":"2025-04-09T14:30:13.326839Z","shell.execute_reply.started":"2025-04-09T14:29:37.190384Z","shell.execute_reply":"2025-04-09T14:30:13.326107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.layers import Dropout, Dense\nfrom keras.regularizers import l2\nfrom keras.initializers import VarianceScaling\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.models import load_model\n\ninitializer = VarianceScaling()\n\noptimizers = {\n    'Adam': Adam(learning_rate=4e-4),\n    'SGD': SGD(learning_rate=4e-2, momentum=0.9),\n    'RMSprop': RMSprop(learning_rate=4e-4)\n}\n\n#models = {\n#    \"Adam\": load_model('/kaggle/working/1dCNN_Adam.h5'),\n#    \"SGD\": load_model('/kaggle/working/1dCNN_RMSprop.h5'),\n#    \"RMSprop\": load_model('/kaggle/working/1dCNN_SGD.h5')\n#}\nmodels = {}\n\ndef build_model(optimizer):\n    inputs = tf.keras.layers.Input((data.shape[1], data.shape[2]))  # 4s at 22,050 Hz\n\n    # First Conv1D block\n    x = tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu', kernel_initializer=initializer, kernel_regularizer=l2(0.0001))(inputs)\n    x = tf.keras.layers.MaxPooling1D(3)(x)\n    \n    # Second Conv1D block\n    x = tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu', kernel_initializer=initializer, kernel_regularizer=l2(0.0001))(x)\n    x = tf.keras.layers.MaxPooling1D(pool_size=3, strides=3)(x)\n    \n    # Third Conv1D block\n    x = tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu', kernel_initializer=initializer, kernel_regularizer=l2(0.0001))(x)\n    x = tf.keras.layers.MaxPooling1D(pool_size=3, strides=3)(x)\n    \n    #Fourth Block\n    x = tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu', kernel_initializer=initializer, kernel_regularizer=l2(0.0001))(x)\n    x = tf.keras.layers.MaxPooling1D(pool_size=3, strides=3)(x)\n    \n    # Fifth block\n    x = tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu', kernel_initializer=initializer, kernel_regularizer=l2(0.0001))(x)\n    x = tf.keras.layers.MaxPooling1D(pool_size=3, strides=3)(x)\n    \n    # Sixth block\n    x = tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu', kernel_initializer=initializer, kernel_regularizer=l2(0.0001))(x)\n    x = tf.keras.layers.MaxPooling1D(pool_size=3, strides=3)(x)\n    \n    # Seventh block\n    x = tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu', kernel_initializer=initializer, kernel_regularizer=l2(0.0001))(x)\n    x = tf.keras.layers.MaxPooling1D(pool_size=3, strides=3)(x)\n    \n    # Eighth block\n    x = tf.keras.layers.Conv1D(256, kernel_size=3, activation='relu', kernel_initializer=initializer, kernel_regularizer=l2(0.000125))(x)\n    x = tf.keras.layers.MaxPooling1D(pool_size=3, strides=3)(x)\n    \n    # Ninth block with GlobalAverageMaxpooling\n    x = tf.keras.layers.Conv1D(512, kernel_size=3, activation='relu', kernel_initializer=initializer, kernel_regularizer=l2(0.0001))(x)\n    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n    \n    \n    x = Dense(1024, activation=\"relu\")(x)\n    x = Dense(10, activation='softmax')(x)\n\n    # Build the model\n    model = tf.keras.models.Model(inputs, x)\n    model.compile(optimizer=optimizer,\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    model.summary()\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:30:13.328126Z","iopub.execute_input":"2025-04-09T14:30:13.328458Z","iopub.status.idle":"2025-04-09T14:30:14.704506Z","shell.execute_reply.started":"2025-04-09T14:30:13.328426Z","shell.execute_reply":"2025-04-09T14:30:14.703848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for name, optimizer in optimizers.items():\n    models[name] = build_model(optimizer)\n\nprint(models)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:30:14.705751Z","iopub.execute_input":"2025-04-09T14:30:14.705982Z","iopub.status.idle":"2025-04-09T14:30:16.678421Z","shell.execute_reply.started":"2025-04-09T14:30:14.705963Z","shell.execute_reply":"2025-04-09T14:30:16.677729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data, labels, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:30:16.679367Z","iopub.execute_input":"2025-04-09T14:30:16.679678Z","iopub.status.idle":"2025-04-09T14:30:18.331365Z","shell.execute_reply.started":"2025-04-09T14:30:16.679654Z","shell.execute_reply":"2025-04-09T14:30:18.330302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nresults = {}\nfor name, model in models.items():\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5, verbose=1)\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n    print('Training model:', name)\n    history = model.fit(X_train, y_train, \n                        validation_data=(X_test, y_test), \n                        epochs=50, \n                        batch_size=64, \n                        callbacks=[early_stopping, reduce_lr])\n    results[name] = history\n    \n    # Save history\n    with open(f'/kaggle/working/{name}_history.pkl', 'wb') as file:\n        pickle.dump(history.history, file)\n    \n    # Save model\n    model.save(f'/kaggle/working/{name}_model.h5')\n    print(f\"Model and history for {name} saved successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T14:30:18.332455Z","iopub.execute_input":"2025-04-09T14:30:18.332814Z","iopub.status.idle":"2025-04-09T16:39:05.770137Z","shell.execute_reply.started":"2025-04-09T14:30:18.332754Z","shell.execute_reply":"2025-04-09T16:39:05.769350Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_optimizer_comparison(histories_dict):\n    # Dictionary to store reformatted histories\n    reformatted_histories = {}\n    \n    # Convert the history objects to the format expected by the original function\n    for name, history_obj in histories_dict.items():\n        reformatted_histories[name] = {'history': history_obj.history}\n    \n    # Continue with your original function logic using the reformatted data\n    # Step 2: Plot training and validation metrics\n    plt.figure(figsize=(14, 5))\n    \n    # Plot 1: Validation Accuracy\n    plt.subplot(1, 2, 1)\n    for name, metrics in reformatted_histories.items():\n        plt.plot(metrics['history']['val_accuracy'], label=f\"{name} Val Acc\")\n    plt.title('Validation Accuracy per Epoch')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    # Plot 2: Validation Loss\n    plt.subplot(1, 2, 2)\n    for name, metrics in reformatted_histories.items():\n        plt.plot(metrics['history']['val_loss'], label=f\"{name} Val Loss\")\n    plt.title('Validation Loss per Epoch')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T16:39:22.998702Z","iopub.execute_input":"2025-04-09T16:39:22.999126Z","iopub.status.idle":"2025-04-09T16:39:23.007647Z","shell.execute_reply.started":"2025-04-09T16:39:22.999087Z","shell.execute_reply":"2025-04-09T16:39:23.006797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a dictionary with your model histories\nhistories = {}\n\n# Load each history file\nfor optimizer in [\"Adam\", \"SGD\", \"RMSprop\"]:\n    try:\n        # Open and load the pickle file\n        with open(f'/kaggle/working/{optimizer}_history.pkl', 'rb') as file:\n            # Create a simple object with a history attribute\n            history_obj = type('', (), {})()\n            history_obj.history = pickle.load(file)\n            \n            # Add to the dictionary\n            histories[optimizer] = history_obj\n            \n        print(f\"Successfully loaded history for {optimizer}\")\n    except Exception as e:\n        print(f\"Failed to load history for {optimizer}: {e}\")\n\nplot_optimizer_comparison(histories)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T16:39:29.280970Z","iopub.execute_input":"2025-04-09T16:39:29.281256Z","iopub.status.idle":"2025-04-09T16:39:29.781840Z","shell.execute_reply.started":"2025-04-09T16:39:29.281234Z","shell.execute_reply":"2025-04-09T16:39:29.780849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\npredictions = {}\n\nfor name, model in models.items():\n    # Get predictions\n    y_pred = model.predict(X_test)\n    \n    # Convert probabilities to class labels\n    y_pred_classes = np.argmax(y_pred, axis=1)\n\n    predictions[name] = {\n        'predictons': y_pred,\n        'predicted_classes': y_pred_classes\n    }\n    \n# Convert one-hot true labels to class labels\ny_true_classes = np.argmax(y_test, axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T16:39:58.307168Z","iopub.execute_input":"2025-04-09T16:39:58.307505Z","iopub.status.idle":"2025-04-09T16:40:21.785688Z","shell.execute_reply.started":"2025-04-09T16:39:58.307475Z","shell.execute_reply":"2025-04-09T16:40:21.784990Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\nfor name, y_pred in predictions.items():\n    print(classification_report(y_true_classes, y_pred['predicted_classes'], target_names=classes))\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true_classes, y_pred['predicted_classes'])\n    \n    # Plot confusion matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.title(f\"Confusion Matrix  1D CNN with {name}\")\n    plt.savefig(f'1dcnn{name}_confusion_matrix.png')\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T16:43:59.283922Z","iopub.execute_input":"2025-04-09T16:43:59.284361Z","iopub.status.idle":"2025-04-09T16:44:01.349045Z","shell.execute_reply.started":"2025-04-09T16:43:59.284295Z","shell.execute_reply":"2025-04-09T16:44:01.348156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}