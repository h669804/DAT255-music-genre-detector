{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T08:28:16.124447Z",
     "iopub.status.busy": "2025-04-11T08:28:16.124136Z",
     "iopub.status.idle": "2025-04-11T08:28:34.868483Z",
     "shell.execute_reply": "2025-04-11T08:28:34.867496Z",
     "shell.execute_reply.started": "2025-04-11T08:28:16.124414Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets\n",
    "Load both the 30s and 3s features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T08:28:34.870121Z",
     "iopub.status.busy": "2025-04-11T08:28:34.869438Z",
     "iopub.status.idle": "2025-04-11T08:28:35.280129Z",
     "shell.execute_reply": "2025-04-11T08:28:35.278816Z",
     "shell.execute_reply.started": "2025-04-11T08:28:34.870090Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "file_path_30sec = \"/kaggle/input/gtzan-dataset-music-genre-classification/Data/features_30_sec.csv\"\n",
    "file_path_3sec = \"/kaggle/input/gtzan-dataset-music-genre-classification/Data/features_3_sec.csv\"\n",
    "\n",
    "df_30sec = pd.read_csv(file_path_30sec)\n",
    "print(f\"30-sec dataset loaded with shape: {df_30sec.shape}\")\n",
    "\n",
    "# Try loading the 3-sec dataset if it exists\n",
    "try:\n",
    "    df_3sec = pd.read_csv(file_path_3sec)\n",
    "    print(f\"3-sec dataset loaded with shape: {df_3sec.shape}\")\n",
    "    use_combined = True\n",
    "except:\n",
    "    print(\"3-sec dataset not found or couldn't be loaded. Using only 30-sec dataset.\")\n",
    "    use_combined = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Includes merging the 30s and 3s features into a single dataset. To implement this, we used a **label encoder** and scaled the data appropiately to avoid large differences between the 30s features and the 3s features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T08:28:35.282033Z",
     "iopub.status.busy": "2025-04-11T08:28:35.281610Z",
     "iopub.status.idle": "2025-04-11T08:28:35.288322Z",
     "shell.execute_reply": "2025-04-11T08:28:35.287084Z",
     "shell.execute_reply.started": "2025-04-11T08:28:35.281989Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data preparation function\n",
    "def prepare_data(df):\n",
    "    # Drop filename column if it exists\n",
    "    if 'filename' in df.columns:\n",
    "        df = df.drop(columns=['filename'])\n",
    "    \n",
    "    # Separate features and labels\n",
    "    X = df.drop(columns=['label']).values\n",
    "    y = df['label'].values\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T08:28:35.291121Z",
     "iopub.status.busy": "2025-04-11T08:28:35.290801Z",
     "iopub.status.idle": "2025-04-11T08:28:35.459960Z",
     "shell.execute_reply": "2025-04-11T08:28:35.458822Z",
     "shell.execute_reply.started": "2025-04-11T08:28:35.291092Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "print(\"Preparing 30-sec dataset...\")\n",
    "X_30sec, y_30sec = prepare_data(df_30sec)\n",
    "print(f\"30-sec data: X shape: {X_30sec.shape}, y shape: {y_30sec.shape}\")\n",
    "\n",
    "# Encode labels consistently (fit on 30sec labels first to ensure consistency)\n",
    "print(\"Encoding labels...\")\n",
    "label_encoder = LabelEncoder()\n",
    "y_30sec_encoded = label_encoder.fit_transform(y_30sec)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Classes: {label_encoder.classes_}\")\n",
    "\n",
    "# Initialize scalers\n",
    "scaler_30sec = StandardScaler()\n",
    "\n",
    "# Scale features\n",
    "print(\"Standardizing 30-sec features...\")\n",
    "X_30sec_scaled = scaler_30sec.fit_transform(X_30sec)\n",
    "\n",
    "if use_combined:\n",
    "    print(\"Preparing 3-sec dataset...\")\n",
    "    X_3sec, y_3sec = prepare_data(df_3sec)\n",
    "    print(f\"3-sec data: X shape: {X_3sec.shape}, y shape: {y_3sec.shape}\")\n",
    "    \n",
    "    # Use the same encoder for 3-sec labels\n",
    "    y_3sec_encoded = label_encoder.transform(y_3sec)\n",
    "    \n",
    "    # Check if feature dimensions match\n",
    "    if X_3sec.shape[1] != X_30sec.shape[1]:\n",
    "        print(f\"Warning: Feature dimensions don't match! 30-sec: {X_30sec.shape[1]}, 3-sec: {X_3sec.shape[1]}\")\n",
    "        print(\"Using only 30-sec dataset\")\n",
    "        use_combined = False\n",
    "    else:\n",
    "        # Scale 3-sec features using 30-sec scaler\n",
    "        print(\"Standardizing 3-sec features...\")\n",
    "        X_3sec_scaled = scaler_30sec.transform(X_3sec)\n",
    "        \n",
    "        # Combine datasets\n",
    "        print(\"Combining datasets...\")\n",
    "        X_combined = np.vstack([X_30sec_scaled, X_3sec_scaled])\n",
    "        y_combined_encoded = np.concatenate([y_30sec_encoded, y_3sec_encoded])\n",
    "        print(f\"Combined dataset: {X_combined.shape} features, {len(y_combined_encoded)} labels\")\n",
    "\n",
    "# Set input dimension and data to use\n",
    "input_dim = X_30sec_scaled.shape[1]\n",
    "if use_combined:\n",
    "    X_data = X_combined\n",
    "    y_data = y_combined_encoded\n",
    "    print(\"Using combined dataset for training\")\n",
    "else:\n",
    "    X_data = X_30sec_scaled\n",
    "    y_data = y_30sec_encoded\n",
    "    print(\"Using only 30-sec dataset for training\")\n",
    "\n",
    "# Split data\n",
    "print(\"Splitting data...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, random_state=42, stratify=y_data\n",
    ")\n",
    "print(f\"Train set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "\n",
    "# Convert to categorical for loss function compatibility\n",
    "print(\"Converting labels to categorical...\")\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "print(\"Conversion complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "Includes a function to define model with self-attention and residual connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T08:28:35.462303Z",
     "iopub.status.busy": "2025-04-11T08:28:35.461906Z",
     "iopub.status.idle": "2025-04-11T08:28:36.249640Z",
     "shell.execute_reply": "2025-04-11T08:28:36.248424Z",
     "shell.execute_reply.started": "2025-04-11T08:28:35.462263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizers = {\n",
    "    \"Adam\": keras.optimizers.Adam(learning_rate=0.001),\n",
    "    \"SGD\": keras.optimizers.SGD(learning_rate=0.004, momentum=0.9),\n",
    "    'RMSprop': keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "}\n",
    "\n",
    "models = {}\n",
    "\n",
    "def build_advanced_model(input_dim, num_classes, optimizer):\n",
    "    # Input layer\n",
    "    inputs = keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    # Self-attention mechanism\n",
    "    # Reshape for attention\n",
    "    attention_input = keras.layers.Reshape((input_dim, 1))(inputs)\n",
    "    \n",
    "    # Multi-head attention layer\n",
    "    attention = keras.layers.MultiHeadAttention(\n",
    "        num_heads=4, key_dim=4\n",
    "    )(attention_input, attention_input)\n",
    "    \n",
    "    # Reshape back to original dimensions\n",
    "    attention = keras.layers.Reshape((input_dim,))(attention[:, :, 0])\n",
    "    \n",
    "    # Combine original input with attention features\n",
    "    x = keras.layers.Concatenate()([inputs, attention])\n",
    "    \n",
    "    # First dense block\n",
    "    x = keras.layers.Dense(512, activation='selu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    \n",
    "    # First residual block\n",
    "    residual = x\n",
    "    x = keras.layers.Dense(512, activation='selu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(512, activation='selu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.add([x, residual])  # Skip connection\n",
    "    \n",
    "    # Second dense block\n",
    "    x = keras.layers.Dense(256, activation='selu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Second residual block\n",
    "    residual = x\n",
    "    x = keras.layers.Dense(256, activation='selu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.3)(x)\n",
    "    x = keras.layers.Dense(256, activation='selu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.add([x, residual])  # Skip connection\n",
    "    \n",
    "    # Third dense block\n",
    "    x = keras.layers.Dense(128, activation='selu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "for name, optimizer in optimizers.items():\n",
    "    models[name] = build_advanced_model(input_dim, num_classes, optimizer)\n",
    "    models[name].summary()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixing and splitting data into training and test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T08:28:36.251041Z",
     "iopub.status.busy": "2025-04-11T08:28:36.250736Z",
     "iopub.status.idle": "2025-04-11T08:28:36.262728Z",
     "shell.execute_reply": "2025-04-11T08:28:36.261470Z",
     "shell.execute_reply.started": "2025-04-11T08:28:36.251006Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define mixup data augmentation\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"Performs mixup augmentation on the batch.\"\"\"\n",
    "    batch_size = len(x)\n",
    "    weights = np.random.beta(alpha, alpha, batch_size)\n",
    "    \n",
    "    # Reshape weights to allow broadcasting\n",
    "    weights = weights.reshape(batch_size, 1)\n",
    "    \n",
    "    # Create pairs of samples\n",
    "    index = np.random.permutation(batch_size)\n",
    "    x1, x2 = x, x[index]\n",
    "    y1, y2 = y, y[index]\n",
    "    \n",
    "    # Generate mixed samples\n",
    "    x_mixed = x1 * weights + x2 * (1 - weights)\n",
    "    y_mixed = y1 * weights + y2 * (1 - weights)\n",
    "    \n",
    "    return x_mixed, y_mixed\n",
    "\n",
    "# Custom training generator with mixup\n",
    "class MixupGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, x, y, batch_size=32, alpha=0.2, shuffle=True):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(x))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_x = self.x[batch_indices]\n",
    "        batch_y = self.y[batch_indices]\n",
    "        \n",
    "        # Apply mixup\n",
    "        batch_x, batch_y = mixup_data(batch_x, batch_y, self.alpha)\n",
    "        \n",
    "        return batch_x, batch_y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "# Use mixup generator for training\n",
    "print(\"Setting up mixup generator...\")\n",
    "train_generator = MixupGenerator(\n",
    "    X_train, y_train_cat, batch_size=32, alpha=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model\n",
    "This model will be using callback functions: EarlyStopping and ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T08:28:36.264116Z",
     "iopub.status.busy": "2025-04-11T08:28:36.263817Z",
     "iopub.status.idle": "2025-04-11T08:54:07.227501Z",
     "shell.execute_reply": "2025-04-11T08:54:07.226164Z",
     "shell.execute_reply.started": "2025-04-11T08:28:36.264088Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    "    )\n",
    "\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=7,\n",
    "        min_lr=0.00001,\n",
    "        verbose=1\n",
    "    )\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=100,     # More epochs for better convergence\n",
    "        validation_data=(X_test, y_test_cat),\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1       # Progress bar\n",
    "    )\n",
    "\n",
    "    results[name] = history\n",
    "    # Save history\n",
    "    with open(f'/kaggle/working/{name}_history.pkl', 'wb') as file:\n",
    "        pickle.dump(history.history, file)\n",
    "    \n",
    "    # Save model\n",
    "    model.save(f'/kaggle/working/{name}_model.h5')\n",
    "    print(f\"Model and history for {name} saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T08:54:07.229151Z",
     "iopub.status.busy": "2025-04-11T08:54:07.228759Z",
     "iopub.status.idle": "2025-04-11T08:54:07.236991Z",
     "shell.execute_reply": "2025-04-11T08:54:07.235643Z",
     "shell.execute_reply.started": "2025-04-11T08:54:07.229113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_optimizer_comparison(histories_dict):\n",
    "    # Dictionary to store reformatted histories\n",
    "    reformatted_histories = {}\n",
    "    \n",
    "    # Convert the history objects to the format expected by the original function\n",
    "    for name, history_obj in histories_dict.items():\n",
    "        reformatted_histories[name] = {'history': history_obj.history}\n",
    "    \n",
    "    # Continue with your original function logic using the reformatted data\n",
    "    # Step 2: Plot training and validation metrics\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Validation Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for name, metrics in reformatted_histories.items():\n",
    "        plt.plot(metrics['history']['val_accuracy'], label=f\"{name} Val Acc\")\n",
    "    plt.title('Validation Accuracy per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 2: Validation Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for name, metrics in reformatted_histories.items():\n",
    "        plt.plot(metrics['history']['val_loss'], label=f\"{name} Val Loss\")\n",
    "    plt.title('Validation Loss per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T08:54:07.238989Z",
     "iopub.status.busy": "2025-04-11T08:54:07.238573Z",
     "iopub.status.idle": "2025-04-11T08:54:07.945389Z",
     "shell.execute_reply": "2025-04-11T08:54:07.944067Z",
     "shell.execute_reply.started": "2025-04-11T08:54:07.238949Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_optimizer_comparison(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-11T09:16:12.756840Z",
     "iopub.status.busy": "2025-04-11T09:16:12.756471Z",
     "iopub.status.idle": "2025-04-11T09:16:12.770745Z",
     "shell.execute_reply": "2025-04-11T09:16:12.769525Z",
     "shell.execute_reply.started": "2025-04-11T09:16:12.756808Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "\n",
    "def evaluate_model(model):\n",
    "    try:\n",
    "        print(f\"Evaluating FCNN model with optimizer: {name}\")\n",
    "        test_loss, test_acc = model.evaluate(X_test, y_test_cat, verbose=1)\n",
    "        print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "    \n",
    "    # Generate predictions, compute evaluation report and confusion matrix\n",
    "    try:\n",
    "        print(\"Generating predictions...\")\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        y_test_classes = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "        print(\"\\nClassification Report:\")\n",
    "        report = classification_report(y_test_classes, y_pred_classes, \n",
    "                                      target_names=label_encoder.classes_)\n",
    "        print(report)\n",
    "        \n",
    "        cm = tf.math.confusion_matrix(y_test_classes, y_pred_classes).numpy()\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=label_encoder.classes_,\n",
    "                   yticklabels=label_encoder.classes_)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'Confusion Matrix for FCNN with {name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'fcnn_{name}_confusion_matrix.png')\n",
    "        \n",
    "        # Calculate per-genre accuracy\n",
    "        genre_acc = {}\n",
    "        for i, genre in enumerate(label_encoder.classes_):\n",
    "            genre_indices = y_test_classes == i\n",
    "            if np.sum(genre_indices) > 0:  # Avoid division by zero\n",
    "                genre_acc[genre] = np.mean(y_pred_classes[genre_indices] == i)\n",
    "        \n",
    "        # Print per-genre accuracy\n",
    "        print(\"\\nPer-genre accuracy:\")\n",
    "        for genre, acc in genre_acc.items():\n",
    "            print(f\"{genre}: {acc:.3f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction/visualization: {e}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "# Save the scaler for later use\n",
    "import joblib\n",
    "joblib.dump(scaler_30sec, 'feature_scaler.joblib')\n",
    "joblib.dump(label_encoder, 'label_encoder.joblib')\n",
    "print(\"Scaler and encoder saved\")\n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T09:16:15.266626Z",
     "iopub.status.busy": "2025-04-11T09:16:15.266213Z",
     "iopub.status.idle": "2025-04-11T09:16:22.516552Z",
     "shell.execute_reply": "2025-04-11T09:16:22.515432Z",
     "shell.execute_reply.started": "2025-04-11T09:16:15.266593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 568973,
     "sourceId": 1032238,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
